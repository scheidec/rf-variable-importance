---
title: "RF Variable Importance Metrics"
author: "Caleb Scheidel"
date: "2018/05/18"
output:
  xaringan::moon_reader:
    css: ["mc-xaringan.css", "mc-xaringan-fonts.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

layout: true
background-color: #fafaef
<div class="my-footer"><img src="mc_logo_rectangle.png" style="height: 30px;"/></div>

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ranger)
library(knitr)
library(caret)
library(party)
library(skimr)
```

---

## Random Forest Variable Importance

- Random forests are typically used as "black box" models for prediction
    - but they do return relative importance metrics associated with each feature in the model
        - these can be used to help interpretability
            - which features are powering the predictions
            - assist in feature selection in "small n, large p" (high dimensional) data

<br>

- Careful attention should be paid to the data you are working with and when it is appropriate to use and interpret the different variable importance metrics from random forests.

---

## Words of caution

- A recent [blog post](http://parrt.cs.usfca.edu/doc/rf-importance/index.html) from a team at USF shows that default importance strategies in both R (randomForest) and Python (scikit) are unreliable in many data scenarios
    - importance metrics are biased when potential predictor variables vary in their scale of measurement or their number of categories

<br>
    
- It is also known that importance metrics are biased when predictor variables are highly correlated.
    - Suboptimal predictor variables may be artificially preferred

---

## Words of caution

- This has actually been known for over ten years (Strobl et al, 2007 and Strobl et al, 2008), but it can be easy to assume the default importances of popular packages will fit your unique datasets.  

- The papers and blog post demonstrate how continuous and high cardinality variables (categorical variables with more categories) are preferred, even if they are equally uninformative compared to variables with less categories

- The authors suggest using _permutation importance_ instead of the default in these cases.

---

## Variable Importance Measures

- Mean decrease in impurity (Gini) (default in randomForest and scikit)
- Permutation importance
- Drop-column importance
- Conditional importance

---

## Mean decrease in impurity (Gini) importance

- Describes the improvement in the "Gini gain" splitting criterion (for classification only)
    - incorporates a "weighted" mean of the individual trees' improvement in the splitting criterion produced by each variable
    - What is "Gini impurity index"?
        - Defined as 
        
        $$\sum_{i=1}^{n_c} p_i(1 - p_i)$$

        where $n_c$ is the number of classes in the target variable and $p_i$ is the ratio of this class

Note: For regression, the analagous metric to the Gini index would be the RSS (residual sum of squares)
        
---

## Mean decrease in impurity (Gini) importance

- Decrease in impurity importance, $I$, is defined as:

$$I = G_{parent} - G_{split1} - G_{split2}$$

averaged over all splits in the forest involving the variable in question results in the mean decrease in impurity importance.

- Pro: Not very computationally expensive
- Con: Biased in favor of continuous and high cardinality variables

---

## Why is impurity importance biased?

- Each time a break point is selected in a variable, every level of the variable is tested to find the best break point
    - Continuous or high cardinality variables will have _many_ more split points
        - "multiple testing" problem
        - higher probability that by chance, that variable happens to predict the outcome well
        - variables where more splits are tried, will appear more often in the tree

---

## Rent data

```{r, echo = FALSE}

# RF Classification - feature importances

library(tidyverse)
library(randomForest)
library(cowplot)
library(gridExtra)

rent <- read.csv('../rent.csv')

rent <- rent %>% 
  mutate(bathrooms = as.factor(bathrooms),
         bedrooms  = as.factor(bedrooms),
         interest_level = as.factor(interest_level))
```

```{r}

rent %>% 
  glimpse()
```

There are 15 unique categories for bathrooms, and 9 unique categories for bedrooms.

Outcome is `interest_level`, with three categories: low (1), medium (2), high (3)

Note: [Data](https://raw.githubusercontent.com/parrt/random-forest-importances/master/notebooks/data/rent.csv) and [code](https://github.com/parrt/random-forest-importances/blob/master/notebooks/permutation-importances-classifier.Rmd) used here is from the authors of the blog post at USF.

---

```{r, echo = FALSE}
## plotting functions ##

create_rfplot <- function(rf, type){
  imp <- importance(rf, type=type, scale = F)
  featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])
  
  p <- ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
       geom_bar(stat="identity", fill="#53cfff", width = 0.65) +
       coord_flip() + 
       theme_light(base_size=20) +
       theme(axis.title.x=element_blank(),
             axis.title.y=element_blank(),
             axis.text.x = element_text(size = 15, color = "black"),
             axis.text.y = element_text(size = 15, color = "black")) 
  return(p)
}

create_ggplot <- function(featureImportance){
  p <- ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
       geom_bar(stat="identity", fill="#53cfff", width = 0.65) +
       coord_flip() + 
       theme_light(base_size=20) +
       theme(axis.title.x=element_blank(),
             axis.title.y=element_blank(),
             axis.text.x = element_text(size = 15, color = "black"),
             axis.text.y = element_text(size = 15, color = "black")) 
  return(p)
}
```

What are the mean decrease in gini importance rankings of these features?

In `randomForest` with `type = 2`:

```{r}

set.seed(1)

rf1 <- randomForest(
  interest_level ~ .,  
  ntree = 40,
  data = rent[, 1:6],
  nodesize = 1, 
  replace = FALSE,
  importance = TRUE
)
```

Note: In `ranger` would use `type = "impurity"`.

---

```{r}
create_rfplot(rf1, type = 2)
```

---

Create a continuous "random" column, which _should_ be last in importance.  

```{r}

rent <- rent %>% 
  mutate(random = sample(100, size = nrow(rent), replace = TRUE))

rf2 = randomForest(
  interest_level ~ .,  
  ntree = 40,
  data = rent, # with random column
  nodesize = 1, 
  replace = FALSE,
  importance = TRUE
)

```

How does mean decrease in Gini rank this variable?

---

```{r}

create_rfplot(rf2, type = 2)
```

---

## Permutation importance

- Basic idea: Consider a variable important if it has a positive effect on the:
    - prediction accuracy (classification)
    - MSE (regression)

- Steps:
    1. Grow a first tree, and calculate the prediction accuracy in the OOB observations
    2. Any association between the variable of interest, $X_i$, and the outcome is broken by permuting the values of all individuals for $X_i$, and the prediction accuracy is computed again.
    3. The difference between the two accuracy values is the permutation importance for $X_i$ from a single tree.
    4. The average of all tree importance values in a random forest then gives the permutation importance of this variable.
    5. Repeat for all variables of interest.

---

## Permutation importance

- More computationally expensive, but more reliable.
    - Does not require retraining the model after permuting each column, just re-run the permuted sample through the already-trained model
    
- Applicable to any model, not just RFs
    
- Risk is a potential bias towards collinear predictive variables (more later)

- `type = 1` in `randomForest`, or `type = "permutation"` in `ranger`

---

How does permutation importance rank the original `rent` variables?

```{r}
create_rfplot(rf1, type = 1) # no random column
```

---

How do these rankings change after adding a continuous random column?

```{r}
create_rfplot(rf2, type = 1) # with random column
```

---
    
## To scale or not to scale?

- Does scaling (dividing by SD) help in certain scenarios?

- Raw, unscaled importance has better statistical properties (Strobl et al, 2008)
    - We don't care what the values are _per se_, we only care about the relative strengths
        - scales the values, but does not alter ranks

- `scale.permutation.importance` argument in `ranger`, default is `FALSE`
- `scale` argument in `randomForest::importance()`, default is `TRUE`
    - in plots shown here, `scale = F`

---

## Collinear features

- Mean decrease in impurity and permutation importance computed from RF models _spread_ importance across collinear variables.
    - i.e. if you duplicate a feature and re-evaluate importance, the duplicated feature pulls down the importance of the original, and they are close to equal in importance

- One possible way to deal with this is to  group highly correlated features into "meta-features" for use in importance rankings.
    - Check Spearman's correlation (non-parametric) and group those highly correlated together

- Another method introduced by Strobl et al (2008) is _conditional_ permutation importance


---

## Conditional permutation importance

- Steps to compute:

1. In each tree, compute the OOB-prediction accuracy before the permutation
2. For all variables Z to be conditioned on: Extract the cutpoints that split this variable in the current tree and create a grid by bisecting the sample space in each cutpoint
    - to determine Z, suggested to include only variables whose empirical correlation with the variable of interest $X_j$ exceeds a threshold (i.e. 0.2).  
3. _Within_ this grid permute the values of $X_j$ and compute the OOB-prediction accuracy after permutation
4. The difference between the prediction accuracy before and after the permutation accuracy again gives the importance of $X_j$ for one tree.  The importance of $X_j$ for the forest is again computed as an average over all trees.

- Note: much more computationally expensive

---

## Conditional permutation importance

- `party::cforest()` to fit the model and `party::varimp()` to compute importance metrics
    - default `threshold` in `party::varimp()` is 0.2.

- More details in the original paper (Strobl et al, 2008) [here, page 7](https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-9-307)

---

## Bootstrapping

- Bootstrap sampling _with_ replacement is traditionally employed in random forests
    - Affects variable selection frequencies
    - artifically induces an association between variables
    
- Avoids the bias towards continuous/categorical variables with a high number of categories.
    - When used with subsampling without replacement

- Default in `randomForest` and `cforest` is `replace = TRUE`
    - Examples here leave the default setting
    - Strobl et al (2007) demonstrates how subsampling _without_ replacement affects importance rankings 
        - [Here](https://link.springer.com/content/pdf/10.1186%2F1471-2105-8-25.pdf)
            - Figure 1 (page 6) on figure 5 (page 10), equally uninformative predictors are the same in bottom right
    
---

## Drop-column importance

- Another potential way to decrease bias in importance with collinear features

- Basic idea: Get a baseline performance score as with permutation importance, but then drop a column entirely, retrain the model, and recompute the performance score
    - importance value of a feature is the difference between the baseline and the score from the model missing that feature
    
- Downside: quite computationally expensive, requires repeatedly retraining the model

---

## Time-dependent data

- What if your data is time dependent?
    - by default OOB performance is used to calculate permutation importance
    - would have to find a specific package or write code to perform the calculations ourselves
        - use time-dependent validation sets and variable shuffling
            - to avoid the time-unaware OOB

---

## Conclusions

- In order to be able to reliably interpret the variable importance measures of a random forest, the forest must be built from unbiased classification trees, and sampling should be conducted _without_ replacement.

- What types of features are you using in your model?
    - all continuous or all categorical with the same number of categories, no collinearity
        - then impurity importance should not be biased
    - mix of categorical and continuous with no collinearity
        - permutation importance should not be biased and can be used with the least computational expense
    - mix of variable types and collinearity
        - conditional permutation importance or drop column importance if computational complexity is not a problem

